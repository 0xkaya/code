{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "042c5bfa",
   "metadata": {},
   "source": [
    "# Grid World with SARSA\n",
    "## Problem Setup\n",
    "The grid is the same as before:\n",
    "\n",
    "#### S  -  -  -\n",
    "#### -  T  -  G\n",
    "#### -  -  -  -\n",
    "\n",
    "S: Start state.\n",
    "\n",
    "G: Goal state (reward = +10).\n",
    "\n",
    "T: Trap state (penalty = -10).\n",
    "\n",
    "-: Empty cells (reward = -1 for each step)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4294e",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc92f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23aa49",
   "metadata": {},
   "source": [
    "## Defining the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d442fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 3,4    # Grid dimensions\n",
    "start_state = (0,0) # Starting position\n",
    "goal_state = (1,3)  # Goal position\n",
    "trap_state = (1,1)  # Trap position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471edf6",
   "metadata": {},
   "source": [
    "## Define Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d72c5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.full((rows,cols), -1) # Default step penalty\n",
    "rewards[goal_state] = 10 # Goal reward\n",
    "rewards[trap_state] = -10 # Trap penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3317df",
   "metadata": {},
   "source": [
    "## Possible Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "048881c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"up\",\"down\",\"left\",\"right\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d84f6",
   "metadata": {},
   "source": [
    "## Map actions to coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73f7d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map = {\n",
    "    \"up\":(-1,0),\n",
    "    \"down\":(1,0),\n",
    "    \"left\":(0,-1),\n",
    "    \"right\":(0,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419ffb93",
   "metadata": {},
   "source": [
    "## Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2605578",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((rows,cols,len(actions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcaacfd",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52d366bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.9 # Discount factor\n",
    "epsilon = 0.2 # Exploration rate\n",
    "episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea308e",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dca77243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(state):\n",
    "    \"\"\"Check if the state is within bounds.\"\"\"\n",
    "    r, c = state\n",
    "    return 0 <= r <rows and 0 <= c <cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "333877f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.randint(0,len(actions)-1) # Explore\n",
    "    else:\n",
    "        r, c = state\n",
    "        return np.argmax(Q[r,c,:]) # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ecca316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state,action):\n",
    "    \"\"\"Get the next state based on the action.\"\"\"\n",
    "    r,c =state \n",
    "    dr, dc = action_map[action]\n",
    "    next_state = (r + dc, c+dc)\n",
    "    return next_state if is_valid_state(next_state) else state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810efc4d",
   "metadata": {},
   "source": [
    "# Sarsa Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    state = start_state\n",
    "    r, c = state\n",
    "    action_index = choose_action(state)\n",
    "\n",
    "    while state != goal_state:\n",
    "        # Take action and observe the next state and reward\n",
    "        action = actions[action_index]\n",
    "        next_state = get_next_state(state, action)\n",
    "        nr, nc = next_state\n",
    "        reward = rewards[nr, nc]\n",
    "\n",
    "        # Choose next action\n",
    "        next_action_index = choose_action(next_state)\n",
    "\n",
    "        # Update Q-value using SARSA formula\n",
    "        Q[r, c, action_index] += alpha * (\n",
    "            reward + gamma * Q[nr, nc, next_action_index] - Q[r, c, action_index]\n",
    "        )\n",
    "\n",
    "        # Move to the next state and action\n",
    "        state = next_state\n",
    "        r, c = nr, nc\n",
    "        action_index = next_action_index\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

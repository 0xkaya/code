{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997bce54-863c-423a-9ff7-658f873412f3",
   "metadata": {},
   "source": [
    "# Creating a voice assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b5f1f-b46e-4e86-ae92-d6fe9016f846",
   "metadata": {},
   "source": [
    "**1. Wake word detection**\n",
    "Voice assistants are constantly listening to the audio inputs coming through your device’s microphone, however they only boot into action when a particular ‘wake word’ or ‘trigger word’ is spoken.\n",
    "\n",
    "**2. Speech transcription**\n",
    "The next stage in the pipeline is transcribing the spoken query to text. In practice, transferring audio files from your local device to the Cloud is slow due to the large nature of audio files, so it’s more efficient to transcribe them directly using an automatic speech recognition (ASR) model on-device rather than using a model in the Cloud. The on-device model might be smaller and thus less accurate than one hosted in the Cloud, but the faster inference speed makes it worthwhile since we can run speech recognition in near real-time, our spoken audio utterance being transcribed as we say it.\n",
    "\n",
    "**3. Language model query**\n",
    "Now that we know what the user asked, we need to generate a response! The best candidate models for this task are large language models (LLMs), since they are effectively able to understand the semantics of the text query and generate a suitable response.\n",
    "\n",
    "**4. Synthesise speech**\n",
    "Finally, we’ll use a text-to-speech (TTS) model to synthesise the text response as spoken speech. This is done on-device, but you could feasibly run a TTS model in the Cloud, generating the audio output and transferring it back to the device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1c473-44f7-457b-9930-6607b742d6e5",
   "metadata": {},
   "source": [
    "## Wake word detection\n",
    "The first stage in the voice assistant pipeline is detecting whether the wake word was spoken, and we need to find ourselves an appropriate pre-trained model for this task! You’ll remember from the section on pre-trained models for audio classification that Speech Commands is a dataset of spoken words designed to evaluate audio classification models on 15+ simple command words like \"up\", \"down\", \"yes\" and \"no\", as well as a \"silence\" label to classify no speech. Take a minute to listen through the samples on the datasets viewer on the Hub and re-acquaint yourself with the Speech Commands dataset: datasets viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8e4202-04b4-4c70-9533-8df07d0c32a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"audio-classification\", model=\"MIT/ast-finetuned-speech-commands-v2\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee8ba51-19b2-4dd4-81e6-08caf39b2347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'backward',\n",
       " 1: 'follow',\n",
       " 2: 'five',\n",
       " 3: 'bed',\n",
       " 4: 'zero',\n",
       " 5: 'on',\n",
       " 6: 'learn',\n",
       " 7: 'two',\n",
       " 8: 'house',\n",
       " 9: 'tree',\n",
       " 10: 'dog',\n",
       " 11: 'stop',\n",
       " 12: 'seven',\n",
       " 13: 'eight',\n",
       " 14: 'down',\n",
       " 15: 'six',\n",
       " 16: 'forward',\n",
       " 17: 'cat',\n",
       " 18: 'right',\n",
       " 19: 'visual',\n",
       " 20: 'four',\n",
       " 21: 'wow',\n",
       " 22: 'no',\n",
       " 23: 'nine',\n",
       " 24: 'off',\n",
       " 25: 'three',\n",
       " 26: 'left',\n",
       " 27: 'marvin',\n",
       " 28: 'yes',\n",
       " 29: 'up',\n",
       " 30: 'sheila',\n",
       " 31: 'happy',\n",
       " 32: 'bird',\n",
       " 33: 'go',\n",
       " 34: 'one'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0636a5c6-27ef-4655-807c-155bfaf5f5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.config.id2label[34]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a66b0d-49ca-4ab3-88a0-09536cdce865",
   "metadata": {},
   "source": [
    "Perfect! We can use this name as our wake word for our voice assistant, similar to how “Alexa” is used for Amazon’s Alexa, or “Hey Siri” is used for Apple’s Siri. Of all the possible labels, if the model predicts \"marvin\" with the highest class probability, we can be fairly sure that our chosen wake word has been said.\n",
    "\n",
    "Now we need to define a function that is constantly listening to our device’s microphone input, and continuously passes the audio to the classification model for inference. To do this, we’ll use a handy helper function that comes with 🤗 Transformers called ffmpeg_microphone_live.\n",
    "\n",
    "This function forwards small chunks of audio of specified length chunk_length_s to the model to be classified. To ensure that we get smooth boundaries across chunks of audio, we run a sliding window across our audio with stride chunk_length_s / 6. So that we don’t have to wait for the entire first chunk to be recorded before we start inferring, we also define a minimal temporary audio input length stream_chunk_s that is forwarded to the model before chunk_length_s time is reached.\n",
    "\n",
    "The function ffmpeg_microphone_live returns a generator object, yielding a sequence of audio chunks that can each be passed to the classification model to make a prediction. We can pass this generator directly to the pipeline, which in turn returns a sequence of output predictions, one for each chunk of audio input. We can inspect the class label probabilities for each audio chunk, and stop our wake word detection loop when we detect that the wake word has been spoken.\n",
    "\n",
    "We’ll use a very simple criteria for classifying whether our wake word was spoken: if the class label with the highest probability was our wake word, and this probability exceeds a threshold prob_threshold, we declare that the wake word as having been spoken. Using a probability threshold to gate our classifier this way ensures that the wake word is not erroneously predicted if the audio input is noise, which is typically when the model is very uncertain and all the class label probabilities low. You might want to tune this probability threshold, or explore more sophisticated means for the wake word decision through an entropy (or uncertainty) based metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63a79e52-bdfb-4328-98a4-89b776c662dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.audio_utils import ffmpeg_microphone_live\n",
    "\n",
    "\n",
    "def launch_fn(\n",
    "    wake_word=\"marvin\",\n",
    "    prob_threshold=0.5,\n",
    "    chunk_length_s=2.0,\n",
    "    stream_chunk_s=0.25,\n",
    "    debug=False,\n",
    "):\n",
    "    if wake_word not in classifier.model.config.label2id.keys():\n",
    "        raise ValueError(\n",
    "            f\"Wake word {wake_word} not in set of valid class labels, pick a wake word in the set {classifier.model.config.label2id.keys()}.\"\n",
    "        )\n",
    "\n",
    "    sampling_rate = classifier.feature_extractor.sampling_rate\n",
    "\n",
    "    mic = ffmpeg_microphone_live(\n",
    "        sampling_rate=sampling_rate,\n",
    "        chunk_length_s=chunk_length_s,\n",
    "        stream_chunk_s=stream_chunk_s,\n",
    "    )\n",
    "\n",
    "    print(\"Listening for wake word...\")\n",
    "    for prediction in classifier(mic):\n",
    "        prediction = prediction[0]\n",
    "        if debug:\n",
    "            print(prediction)\n",
    "        if prediction[\"label\"] == wake_word:\n",
    "            if prediction[\"score\"] > prob_threshold:\n",
    "                return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02fb5fb8-3f67-4271-8432-d93ef8bb6d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for wake word...\n",
      "Using microphone: Mikrofon Dizisi (Dijital Mikrofonlar için Intel® Smart Sound Teknolojisi)\n",
      "{'score': 0.967997133731842, 'label': 'yes'}\n",
      "{'score': 0.4539084732532501, 'label': 'two'}\n",
      "{'score': 0.8804502487182617, 'label': 'yes'}\n",
      "{'score': 0.938687801361084, 'label': 'yes'}\n",
      "{'score': 0.8976845145225525, 'label': 'yes'}\n",
      "{'score': 0.8597816824913025, 'label': 'yes'}\n",
      "{'score': 0.8597821593284607, 'label': 'yes'}\n",
      "{'score': 0.8597816824913025, 'label': 'yes'}\n",
      "{'score': 0.4782681465148926, 'label': 'seven'}\n",
      "{'score': 0.9787492156028748, 'label': 'six'}\n",
      "{'score': 0.9834354519844055, 'label': 'six'}\n",
      "{'score': 0.9834354519844055, 'label': 'six'}\n",
      "{'score': 0.9834354519844055, 'label': 'six'}\n",
      "{'score': 0.9834354519844055, 'label': 'six'}\n",
      "{'score': 0.6814892292022705, 'label': 'six'}\n",
      "{'score': 0.8099743723869324, 'label': 'six'}\n",
      "{'score': 0.8099744319915771, 'label': 'six'}\n",
      "{'score': 0.8099743723869324, 'label': 'six'}\n",
      "{'score': 0.8099744319915771, 'label': 'six'}\n",
      "{'score': 0.5548411011695862, 'label': 'six'}\n",
      "{'score': 0.857767641544342, 'label': 'six'}\n",
      "{'score': 0.8099634647369385, 'label': 'six'}\n",
      "{'score': 0.8099633455276489, 'label': 'six'}\n",
      "{'score': 0.8099634647369385, 'label': 'six'}\n",
      "{'score': 0.8099634647369385, 'label': 'six'}\n",
      "{'score': 0.9992349147796631, 'label': 'no'}\n",
      "{'score': 0.9999803304672241, 'label': 'no'}\n",
      "{'score': 0.9999803304672241, 'label': 'no'}\n",
      "{'score': 0.9999803304672241, 'label': 'no'}\n",
      "{'score': 0.9999803304672241, 'label': 'no'}\n",
      "{'score': 0.9315895438194275, 'label': 'four'}\n",
      "{'score': 0.9842961430549622, 'label': 'four'}\n",
      "{'score': 0.9842961430549622, 'label': 'four'}\n",
      "{'score': 0.9842961430549622, 'label': 'four'}\n",
      "{'score': 0.9842961430549622, 'label': 'four'}\n",
      "{'score': 0.10746467113494873, 'label': 'off'}\n",
      "{'score': 0.0758049264550209, 'label': 'off'}\n",
      "{'score': 0.07955403625965118, 'label': 'off'}\n",
      "{'score': 0.07955407351255417, 'label': 'off'}\n",
      "{'score': 0.07955403625965118, 'label': 'off'}\n",
      "{'score': 0.07955403625965118, 'label': 'off'}\n",
      "{'score': 0.9963836669921875, 'label': 'follow'}\n",
      "{'score': 0.9992380142211914, 'label': 'follow'}\n",
      "{'score': 0.9992380142211914, 'label': 'follow'}\n",
      "{'score': 0.9992380142211914, 'label': 'follow'}\n",
      "{'score': 0.9992380142211914, 'label': 'follow'}\n",
      "{'score': 0.9999868869781494, 'label': 'follow'}\n",
      "{'score': 0.999988317489624, 'label': 'follow'}\n",
      "{'score': 0.999988317489624, 'label': 'follow'}\n",
      "{'score': 0.999988317489624, 'label': 'follow'}\n",
      "{'score': 0.999988317489624, 'label': 'follow'}\n",
      "{'score': 0.5536928176879883, 'label': 'six'}\n",
      "{'score': 0.7358333468437195, 'label': 'six'}\n",
      "{'score': 0.6546815633773804, 'label': 'six'}\n",
      "{'score': 0.6546812653541565, 'label': 'six'}\n",
      "{'score': 0.6546815633773804, 'label': 'six'}\n",
      "{'score': 0.6546815633773804, 'label': 'six'}\n",
      "{'score': 0.9995244741439819, 'label': 'yes'}\n",
      "{'score': 0.99982088804245, 'label': 'yes'}\n",
      "{'score': 0.99982088804245, 'label': 'yes'}\n",
      "{'score': 0.99982088804245, 'label': 'yes'}\n",
      "{'score': 0.99982088804245, 'label': 'yes'}\n",
      "{'score': 0.9994997978210449, 'label': 'yes'}\n",
      "{'score': 0.9996182918548584, 'label': 'yes'}\n",
      "{'score': 0.9996182918548584, 'label': 'yes'}\n",
      "{'score': 0.9996182918548584, 'label': 'yes'}\n",
      "{'score': 0.9996182918548584, 'label': 'yes'}\n",
      "{'score': 0.17343108355998993, 'label': 'six'}\n",
      "{'score': 0.9987581968307495, 'label': 'marvin'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "launch_fn(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e858a8-8dfd-4113-80af-9bfa6cff4df4",
   "metadata": {},
   "source": [
    "Awesome! As we expect, the model generates garbage predictions for the first few seconds. There is no speech input, so the model makes close to random predictions, but with very low probability. As soon as we say the wake word, the model predicts \"marvin\" with probability close to 1 and terminates the loop, signalling that the wake word has been detected and that the ASR system should be activated!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db960d34-b48c-4ecc-9d88-fe7ff2077f95",
   "metadata": {},
   "source": [
    "## Speech transcription\n",
    "\n",
    "Once again, we’ll use the Whisper model for our speech transcription system. Specifically, we’ll load the Whisper Base English checkpoint, since it’s small enough to give good inference speed with reasonable transcription accuracy. We’ll use a trick to get near real-time transcription by being clever with how we forward our audio inputs to the model. As before, feel free to use any speech recognition checkpoint on the Hub, including Wav2Vec2, MMS ASR or other Whisper checkpoints:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2085a43c-e564-43e4-800f-82174878fa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa2397b29664c4bb376db49a9933411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furka\\miniconda3\\envs\\audio\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\furka\\.cache\\huggingface\\hub\\models--openai--whisper-base.en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\furka\\miniconda3\\envs\\audio\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670ab94b413a4d89b10dabaa257fa1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c63567ae12416c998ac817f61ccc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a7f6913e2644beba96d06a1bf8ee2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5beb2005d2ec4f83bd2aa69b1256dc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a810a03c014c2eacef1199d82c8424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5303a5bd42dc4cec85dfe1b2fd514c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23c432511154531a1797a977c77130a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bd031e01014b28860988e624cd6f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb805c11d354e4ea8f5a617c3847726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e301ee3a8e41498646fb9ea39b7554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-base.en\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba27ec-442a-4b43-86c4-1ffe0e9ad11b",
   "metadata": {},
   "source": [
    "We can now define a function to record our microphone input and transcribe the corresponding text. With the ffmpeg_microphone_live helper function, we can control how ‘real-time’ our speech recognition model is. Using a smaller stream_chunk_s lends itself to more real-time speech recognition, since we divide our input audio into smaller chunks and transcribe them on the fly. However, this comes at the expense of poorer accuracy, since there’s less context for the model to infer from.\n",
    "\n",
    "As we’re transcribing the speech, we also need to have an idea of when the user stops speaking, so that we can terminate the recording. For simplicity, we’ll terminate our microphone recording after the first chunk_length_s (which is set to 5 seconds by default), but you can experiment with using a voice activity detection (VAD) model to predict when the user has stopped speaking.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "193b78e3-6d69-42e2-a417-5b9881925e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def transcribe(chunk_length_s=10.0, stream_chunk_s=1.0):\n",
    "    sampling_rate = transcriber.feature_extractor.sampling_rate\n",
    "\n",
    "    mic = ffmpeg_microphone_live(\n",
    "        sampling_rate=sampling_rate,\n",
    "        chunk_length_s=chunk_length_s,\n",
    "        stream_chunk_s=stream_chunk_s,\n",
    "    )\n",
    "\n",
    "    print(\"Start speaking...\")\n",
    "    for item in transcriber(mic, generate_kwargs={\"max_new_tokens\": 128}):\n",
    "        sys.stdout.write(\"\\033[K\")\n",
    "        print(item[\"text\"], end=\"\\r\")\n",
    "        if not item[\"partial\"][0]:\n",
    "            break\n",
    "\n",
    "    return item[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9517d96-d128-485c-8df0-6366367fd52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start speaking...\n",
      "Using microphone: Mikrofon Dizisi (Dijital Mikrofonlar için Intel® Smart Sound Teknolojisi)\n",
      "\u001b[K I am trying to build the voice assistant system for my master thesis.\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I am trying to build the voice assistant system for my master thesis.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62092d8b-569e-481d-b822-785493f3a75f",
   "metadata": {},
   "source": [
    "## Language model query\n",
    "Now that we have our spoken query transcribed, we want to generate a meaningful response. To do this, we’ll use an LLM hosted on the Cloud. Specifically, we’ll pick an LLM on the Hugging Face Hub and use the Inference API to easily query the model.\n",
    "\n",
    "First, let’s head over to the Hugging Face Hub. To find our LLM, we’ll use the 🤗 Open LLM Leaderboard, a Space that ranks LLM models by performance over four generation tasks. We’ll search by “instruct” to filter out models that have been instruction fine-tuned, since these should work better for our querying task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bbb70-f6f0-4008-b9c1-bbfb5c9a3794",
   "metadata": {},
   "source": [
    "The Inference API allows us to send a HTTP request from our local machine to the LLM hosted on the Hub, and returns the response as a json file. All we need to provide is our Hugging Face Hub token (which we retrieve directly from our Hugging Face Hub folder) and the model id of the LLM we wish to query:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d54e1ff-f23f-4367-8fae-a0cc442b41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "import requests\n",
    "\n",
    "\n",
    "def query(text, model_id=\"tiiuae/falcon-7b-instruct\"):\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HfFolder().get_token()}\"}\n",
    "    payload = {\"inputs\": text}\n",
    "\n",
    "    print(f\"Querying...: {text}\")\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    return response.json()[0][\"generated_text\"][len(text) + 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58dcbf32-a489-4fe1-bb3a-858b6309982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...: What should i do today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I cannot answer that question as it depends on personal preferences and local events or activities. It would be best to consult online resources or inquire with locals for suggestions.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"What should i do today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb87c7-99ae-4574-b23a-360630de1a57",
   "metadata": {},
   "source": [
    "## Synthesise speech\n",
    "\n",
    "And now we’re ready to get the final spoken output! Once again, we’ll use the Microsoft SpeechT5 TTS model for English TTS, but you can use any TTS model of your choice. Let’s go ahead and load the processor and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e220d7-c7e4-41d3-9289-c93c9edf91cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7825dba5-bada-410e-bbfb-58f864f186e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And also the speaker embeddings:\n",
    "from datasets import load_dataset\n",
    "from torch import torch\n",
    "\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46abbf0-319e-47ee-b3bf-0c1727b2ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesise(text):\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    speech = model.generate_speech(\n",
    "        inputs[\"input_ids\"].to(device), speaker_embeddings.to(device), vocoder=vocoder\n",
    "    )\n",
    "    return speech.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "085decfb-7c89-405d-9585-043f73ffb878",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m----> 3\u001b[0m audio \u001b[38;5;241m=\u001b[39m synthesise(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHugging Face is a company that provides natural language processing and machine learning tools for developers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m Audio(audio, rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m, in \u001b[0;36msynthesise\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynthesise\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mtext, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     speech \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_speech(\n\u001b[1;32m----> 4\u001b[0m         inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), speaker_embeddings\u001b[38;5;241m.\u001b[39mto(device), vocoder\u001b[38;5;241m=\u001b[39mvocoder\n\u001b[0;32m      5\u001b[0m     )\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m speech\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "audio = synthesise(\n",
    "    \"Hugging Face is a company that provides natural language processing and machine learning tools for developers.\"\n",
    ")\n",
    "\n",
    "Audio(audio, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
